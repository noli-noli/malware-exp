from keras.models import Sequential
from keras.layers import LSTM, Dense,Dropout,Embedding
import matplotlib.pyplot as plt
import numpy as np

class LSTM_model:
    def __init__(self,hidden_size=32,input_size=(None,None),output_size=1):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

    def build_model(self):
        #フィードフォワードモデルの生成
        model = Sequential()
        #フォワードモデルにLSTM層を追加
        model.add(LSTM(units=self.hidden_size,input_shape=self.input_size, return_sequences=False))
        #2層目のLSTM層を追加
        #model.add(LSTM(units=100,return_sequences=False))
        #LSTM層の後に全結合層を追加。活性化関数をsoftmaxに指定
        model.add(Dense(self.output_size,activation='sigmoid'))
        #モデルをコンパイル
        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])
        #作成したモデルを返す
        return model

class Custom_EarlyStopping:
    def __init__(self,patience):
        self.patience = patience
        self.best_loss = np.inf
        self.wait = 0
    
    def check(self,current_val_loss):
        if current_val_loss < self.best_loss:
            self.best_loss = current_val_loss
            self.wait = 0
        else:
            self.wait += 1
            if self.wait >= self.patience:
                #学習を停止する値を返す。
                return True
            #学習を継続する値値を返す。
            return False

class CustomEmbeddingLayer:
    def __init__(self, input_dim, output_dim):
        self.input_dim = input_dim
        self.output_dim = output_dim
        # Embedding 行列の初期化
        self.embedding_matrix = np.random.rand(input_dim, output_dim)

    def forward(self, input_sequence):
        # 入力シーケンスのembedding
        embedded_sequence = self.embedding_matrix[input_sequence]
        #入力シーケンスに対するランダム行列(即ち固有のエンベディング結果)を返す
        return embedded_sequence

