import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='1'
import pandas as pd
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence

max_words = 1000
max_len = 1500

api_dataset = os.path.join("..","..","dataset","mal-api-2019.txt")
label_dataset = os.path.join("..","..","dataset","mal-label-2019.csv")


##### データセットの読込 #####
with open(api_dataset) as f:
    tmp = f.readlines()
content = [x.strip() for x in tmp]
data = pd.DataFrame()
data['feature'] = content
############################


##### データセットの前処理 #####
X = data["feature"]
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X = tokenizer.texts_to_sequences(X)
X = sequence.pad_sequences(X, truncating='post')
print(tokenizer.word_index)
print(X[0])
#############################


"""
#### ラベルの読込 #####
with open(label_dataset) as f:
    label = f.readlines()

label = [x.strip() for x in label]
data["label"] = label
"""